{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import tensorflow as tf\r\n",
    "from keras import initializers, regularizers, constraints\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.layers import Layer\r\n",
    "import keras.backend as K\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class Attention(Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 W_regularizer=None, b_regularizer=None,\r\n",
    "                 W_constraint=None, b_constraint=None,\r\n",
    "                 bias=True, return_attention=False,\r\n",
    "                 **kwargs):\r\n",
    "        self.supports_masking = True\r\n",
    "        self.return_attention = return_attention\r\n",
    "        self.init = initializers.get('glorot_uniform')\r\n",
    "\r\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\r\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\r\n",
    "\r\n",
    "        self.W_constraint = constraints.get(W_constraint)\r\n",
    "        self.b_constraint = constraints.get(b_constraint)\r\n",
    "\r\n",
    "        self.bias = bias\r\n",
    "        super(Attention, self).__init__(**kwargs)\r\n",
    "\r\n",
    "\r\n",
    "    def build(self, input_shape):\r\n",
    "        assert len(input_shape) == 3\r\n",
    "\r\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\r\n",
    "                                 initializer=self.init,\r\n",
    "                                 name=f'{self.name}_Weight',\r\n",
    "                                 regularizer=self.W_regularizer,\r\n",
    "                                 constraint=self.W_constraint)\r\n",
    "        if self.bias:\r\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\r\n",
    "                                     initializer='zero',\r\n",
    "                                     name=f'{self.name}_bias',\r\n",
    "                                     regularizer=self.b_regularizer,\r\n",
    "                                     constraint=self.b_constraint)\r\n",
    "        else:\r\n",
    "            self.b = None\r\n",
    "\r\n",
    "        self.built = True\r\n",
    "\r\n",
    "    def compute_mask(self, input, input_mask=None):\r\n",
    "        # do not pass the mask to the next layers\r\n",
    "        return None\r\n",
    "\r\n",
    "    def call(self, x, mask=None):\r\n",
    "        eij = K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1)\r\n",
    "\r\n",
    "        if self.bias:\r\n",
    "            eij += self.b\r\n",
    "\r\n",
    "        eij = K.tanh(eij)\r\n",
    "\r\n",
    "        a = K.exp(eij)\r\n",
    "\r\n",
    "        # apply mask after the exp. will be re-normalized next\r\n",
    "        if mask is not None:\r\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\r\n",
    "            a *= K.cast(mask, K.floatx())\r\n",
    "\r\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\r\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\r\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\r\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\r\n",
    "\r\n",
    "        weighted_input = x * K.expand_dims(a)\r\n",
    "\r\n",
    "        result = K.sum(weighted_input, axis=1)\r\n",
    "\r\n",
    "        if self.return_attention:\r\n",
    "            return [result, a]\r\n",
    "        return result\r\n",
    "\r\n",
    "    def compute_output_shape(self, input_shape):\r\n",
    "        if self.return_attention:\r\n",
    "            return [(input_shape[0], input_shape[-1]),\r\n",
    "                    (input_shape[0], input_shape[1])]\r\n",
    "        else:\r\n",
    "            return input_shape[0], input_shape[-1]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from dataclasses import dataclass\r\n",
    "\r\n",
    "@dataclass\r\n",
    "class BiGruSettings:\r\n",
    "    name : str\r\n",
    "    input_shape: tuple[int]\r\n",
    "    n_units: int\r\n",
    "\r\n",
    "@dataclass \r\n",
    "class DistanceSettings:\r\n",
    "    name : str\r\n",
    "    mode: str\r\n",
    "\r\n",
    "@dataclass\r\n",
    "class InnerModelSettings:\r\n",
    "    name: str\r\n",
    "    embedding_input_dim : int\r\n",
    "    embedding_output_dim : int\r\n",
    "    char_level_settings: BiGruSettings\r\n",
    "    word_level_settings: BiGruSettings\r\n",
    "\r\n",
    "@dataclass\r\n",
    "class OuterModelSettings:\r\n",
    "    name: str\r\n",
    "    inner_settings: InnerModelSettings\r\n",
    "    distance_settings: DistanceSettings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "class BiGruWithAttention(Layer):\r\n",
    "    def __init__(self,settings:BiGruSettings,**kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.settings = settings\r\n",
    "        self.gru_layer = tf.keras.layers.GRU(self.settings.n_units,return_sequences=True,name=self.settings.name+\"_gru\")\r\n",
    "        self.bidirectional_layer = tf.keras.layers.Bidirectional(self.gru_layer, input_shape=self.settings.input_shape, name=self.settings.name+\"_BiGru\")\r\n",
    "        self.attention_layer = Attention(return_attention=True, name = self.settings.name+\"_attention_vec\")\r\n",
    "    \r\n",
    "    def __call__(self,x,training=False):\r\n",
    "        x = self.bidirectional_layer(x,training=training)\r\n",
    "        sequence, element_scores = self.attention_layer(x,training=training)\r\n",
    "\r\n",
    "        return sequence\r\n",
    "\r\n",
    "class DistanceLayer(tf.keras.layers.Layer):\r\n",
    "    \"\"\"\r\n",
    "    Layer responsible for computation of cosine similarity\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self,settings:DistanceSettings,**kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.settings = settings\r\n",
    "\r\n",
    "    def call(self,input_a,input_b):\r\n",
    "        if self.settings.mode == \"abs\":\r\n",
    "            dist = tf.math.abs(tf.keras.slosses.cosine_similarity(input_a,input_b))\r\n",
    "        elif self.settings.mode == \"zero_to_one\":\r\n",
    "            dist = ( 1-tf.keras.losses.cosine_similarity(input_a,input_b) ) / 2\r\n",
    "        else:\r\n",
    "            dist = tf.keras.losses.cosine_similarity(input_a,input_b)\r\n",
    "\r\n",
    "        return (dist)\r\n",
    "\r\n",
    "\r\n",
    "class InnerModel(Layer):\r\n",
    "    def __init__(self,settings:InnerModelSettings,**kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "\r\n",
    "        self.settings = settings\r\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(input_dim=self.settings.embedding_input_dim,output_dim=self.settings.embedding_output_dim)\r\n",
    "        self.char_level_attention = BiGruWithAttention(self.settings.char_level_settings)\r\n",
    "        self.word_level_attention = BiGruWithAttention(self.settings.word_level_settings)\r\n",
    "\r\n",
    "    def __call__(self, x, training=False):\r\n",
    "        print(f\"Input: {x}\")\r\n",
    "        x = self.embedding_layer(x,training=training)\r\n",
    "        print(f\"X after embedding: {x}\")\r\n",
    "        x = self.char_level_attention(x)\r\n",
    "        print(f\"X char level attention: {x}\")\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "test_string = [\"Grigory Sharkov\",\"Boris Jonson Junior\"]\r\n",
    "tokenizer = Tokenizer(char_level=True,lower=False)\r\n",
    "tokenizer.fit_on_texts(test_string)\r\n",
    "print(tokenizer.word_index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'o': 1, 'r': 2, 'i': 3, ' ': 4, 'n': 5, 's': 6, 'J': 7, 'G': 8, 'g': 9, 'y': 10, 'S': 11, 'h': 12, 'a': 13, 'k': 14, 'v': 15, 'B': 16, 'u': 17}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from typing import Iterable\r\n",
    "def preprocess_list(lst:Iterable, tokenizer:Tokenizer, max_words:int=None, max_char:int=None):\r\n",
    "    \"\"\"\r\n",
    "    Function preprocesses a given list. A string is turned into a sequence of words of length max_words,\r\n",
    "    then each word is turned into a sequence of characters of length max_char, if any of maximum parameters is \r\n",
    "    not specified, they are calcualted based on the provided list\r\n",
    "\r\n",
    "    function returns a tf.DataSet\r\n",
    "    \"\"\"\r\n",
    "    if max_words is None:\r\n",
    "        word_counts = [len(x.split()) for x in lst]\r\n",
    "        max_words = max(word_counts)\r\n",
    "\r\n",
    "    if max_char is None:\r\n",
    "        words = list(set([word for name in lst for word in name.split()]))\r\n",
    "        word_lengths = [len(word) for word in words]\r\n",
    "        max_char = max(word_lengths)\r\n",
    "\r\n",
    "    padded_test_string = [x + \" \"*(max_words-len(x.split())) for x in lst]\r\n",
    "    test_split = [x.split(\" \") for x in padded_test_string]\r\n",
    "    test_sequences = [tokenizer.texts_to_sequences(x) for x in test_split]\r\n",
    "    padded_test_sequences = [pad_sequences(x,maxlen=max_char,padding=\"post\") for x in test_sequences]\r\n",
    "    return_matrix = tf.data.Dataset.from_tensor_slices(padded_test_sequences)\r\n",
    "    return return_matrix\r\n",
    "\r\n",
    "test_names = preprocess_list(test_string,tokenizer,3,4).batch(2)\r\n",
    "test_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None, 3, 4), types: tf.int32>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "experiment_settings = OuterModelSettings(\r\n",
    "    name=\"two_level_rnn_with_attention\",\r\n",
    "    inner_settings=InnerModelSettings(\r\n",
    "        name=\"inner_model\",\r\n",
    "        embedding_input_dim=len(tokenizer.word_index),\r\n",
    "        embedding_output_dim=5,\r\n",
    "        char_level_settings=BiGruSettings(\r\n",
    "            name=\"char_bigru\",\r\n",
    "            input_shape=(3,4,5),\r\n",
    "            n_units=6\r\n",
    "        ),\r\n",
    "        word_level_settings=BiGruSettings(\r\n",
    "            name=\"word_bigru\",\r\n",
    "            input_shape=(3,6),\r\n",
    "            n_units=7\r\n",
    "        ),\r\n",
    "    ),\r\n",
    "    distance_settings = DistanceSettings(\"distance\",\"zero_to_one\")\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "inner_model = InnerModel(experiment_settings.inner_settings)\r\n",
    "for el in test_names:\r\n",
    "    print(inner_model(el))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: [[[ 9  1  2 10]\n",
      "  [ 2 14  1 15]\n",
      "  [ 0  0  0  0]]\n",
      "\n",
      " [[ 1  2  3  6]\n",
      "  [ 5  6  1  5]\n",
      "  [ 5  3  1  2]]]\n",
      "X after embedding: [[[[ 0.01586528  0.04389241 -0.00771624 -0.01105056  0.01934382]\n",
      "   [ 0.01433593  0.00120996  0.0304316   0.03688076  0.02319697]\n",
      "   [ 0.02539683  0.02100693 -0.0137306   0.01544705  0.03589076]\n",
      "   [ 0.00408237  0.04511049  0.04383305 -0.01469549  0.03973918]]\n",
      "\n",
      "  [[ 0.02539683  0.02100693 -0.0137306   0.01544705  0.03589076]\n",
      "   [-0.01071814 -0.00616956 -0.00104443 -0.0175782   0.01681907]\n",
      "   [ 0.01433593  0.00120996  0.0304316   0.03688076  0.02319697]\n",
      "   [ 0.01163561 -0.02268995 -0.04409087 -0.03580417  0.01963869]]\n",
      "\n",
      "  [[-0.03303801 -0.00533998 -0.00573512  0.02302686  0.03536339]\n",
      "   [-0.03303801 -0.00533998 -0.00573512  0.02302686  0.03536339]\n",
      "   [-0.03303801 -0.00533998 -0.00573512  0.02302686  0.03536339]\n",
      "   [-0.03303801 -0.00533998 -0.00573512  0.02302686  0.03536339]]]\n",
      "\n",
      "\n",
      " [[[ 0.01433593  0.00120996  0.0304316   0.03688076  0.02319697]\n",
      "   [ 0.02539683  0.02100693 -0.0137306   0.01544705  0.03589076]\n",
      "   [ 0.04724062  0.0432076   0.00405694  0.00768074  0.03339887]\n",
      "   [ 0.01537793 -0.0239164  -0.03100981  0.01548847  0.04087135]]\n",
      "\n",
      "  [[-0.00011581 -0.01417483 -0.02947391  0.02535588  0.02716497]\n",
      "   [ 0.01537793 -0.0239164  -0.03100981  0.01548847  0.04087135]\n",
      "   [ 0.01433593  0.00120996  0.0304316   0.03688076  0.02319697]\n",
      "   [-0.00011581 -0.01417483 -0.02947391  0.02535588  0.02716497]]\n",
      "\n",
      "  [[-0.00011581 -0.01417483 -0.02947391  0.02535588  0.02716497]\n",
      "   [ 0.04724062  0.0432076   0.00405694  0.00768074  0.03339887]\n",
      "   [ 0.01433593  0.00120996  0.0304316   0.03688076  0.02319697]\n",
      "   [ 0.02539683  0.02100693 -0.0137306   0.01544705  0.03589076]]]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Input 0 of layer char_bigru_BiGru is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (2, 3, 4, 5)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-ac0346b6f06f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0minner_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInnerModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner_settings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-4f206dbfe80d>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, training)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"X after embedding: {x}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar_level_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"X char level attention: {x}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-4f206dbfe80d>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, training)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbidirectional_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         training=training_mode):\n\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m       \u001b[0minput_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    213\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[0;32m    216\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer char_bigru_BiGru is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (2, 3, 4, 5)"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('nlp_tensor': conda)"
  },
  "interpreter": {
   "hash": "fcbba280eac50286b31d8cb86df25696dcc42d5d7558ed290f125a3371417a84"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}