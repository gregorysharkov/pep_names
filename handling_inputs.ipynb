{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tensorflow as tf\r\n",
    "import pathlib\r\n",
    "import os\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "np.set_printoptions(precision=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "path = \"data\\\\combinations\\\\\"\r\n",
    "true_data = pd.read_csv(path+\"governors_true_match.csv\",sep=\";\")\r\n",
    "false_data = pd.read_csv(path+\"governors_false_match.csv\",sep=\";\")\r\n",
    "combined_data = pd.concat([true_data,false_data])\r\n",
    "names = sorted(set(list(combined_data.governor) + list(combined_data.combinations)))\r\n",
    "words = sorted(set(word for name in list(map(str.split,names)) for word in name))\r\n",
    "vocab = sorted(set(character for word in words for character in word))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "governors_list = list(combined_data.governor)\r\n",
    "combination_list = list(combined_data.combinations)\r\n",
    "match = list(combined_data.match)\r\n",
    "\r\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token=\"UNK\")\r\n",
    "tk.fit_on_texts(governors_list+combination_list)\r\n",
    "\r\n",
    "def preprocess_list(lst,tokenizer,max_len=None):\r\n",
    "    return_seq = tokenizer.texts_to_sequences(lst)\r\n",
    "    return np.array(pad_sequences(return_seq, maxlen=max_len,padding=\"post\"),dtype=\"float32\")\r\n",
    "\r\n",
    "governor_seq = preprocess_list(governors_list,tk,30)\r\n",
    "combination_seq = preprocess_list(combination_list,tk,30)\r\n",
    "features = zip(governor_seq,combination_seq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "from dataclasses import dataclass\r\n",
    "\r\n",
    "@dataclass\r\n",
    "class InnerModelSettings:\r\n",
    "    \"\"\"\r\n",
    "    Dataclass for storring inner model settings\r\n",
    "    \"\"\"\r\n",
    "    n_embedding_dims: int\r\n",
    "    n_gru: int\r\n",
    "    n_dense: int\r\n",
    "\r\n",
    "class InnerModel(tf.keras.Model):\r\n",
    "    \"\"\"\r\n",
    "    Inner model to be used inside the outer model.\r\n",
    "    It is responsible for transformations of a sequence into a vector representation\r\n",
    "    that will be used further for comparisson\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, settings: InnerModelSettings):\r\n",
    "        super().__init__()\r\n",
    "        self.n_embedding_dims = settings.n_embedding_dims\r\n",
    "        self.n_gru = settings.n_gru\r\n",
    "        self.n_dense = settings.n_dense\r\n",
    "\r\n",
    "        self.embedding = tf.keras.layers.Embedding(len(tk.word_index)+1,self.n_embedding_dims,name=\"inner_embedding\")\r\n",
    "        self.gru = tf.keras.layers.GRU(self.n_gru,name=\"gru\")\r\n",
    "        self.bi_gru = tf.keras.layers.Bidirectional(self.gru,name=\"inner_bidirectional\")\r\n",
    "        self.dense = tf.keras.layers.Dense(self.n_dense,name=\"inner_dense\")\r\n",
    "    \r\n",
    "    def call(self, x, training=False):\r\n",
    "        #inputs = inputs.reshape(-1,len(inputs))\r\n",
    "        #x = tf.reshape(x,shape=(-1,len(x)))\r\n",
    "        #x.numpy().reshape(-1,len(x.numpy()))\r\n",
    "        x = self.embedding(x,training=training)\r\n",
    "        x = self.bi_gru(x,training=training)\r\n",
    "        x = self.dense(x,training=training)\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "class DistanceLayer(tf.keras.layers.Layer):\r\n",
    "    \"\"\"\r\n",
    "    Layer responsible for computation of cosine similarity\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self,**kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "\r\n",
    "    def call(self,input_a,input_b):\r\n",
    "        dist = ( 1-tf.keras.losses.cosine_similarity(input_a,input_b) ) / 2\r\n",
    "        return dist#tf.reshape(dist,shape=(len(dist),-1))\r\n",
    "\r\n",
    "class OuterModel(tf.keras.Model):\r\n",
    "    \"\"\"\r\n",
    "    Outer model. It takes two inputs (one sequence for each string compared)\r\n",
    "    trahsorms them into a vector representations using inner model\r\n",
    "    and comptes the output using the cosine distance layer\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, settings: InnerModelSettings):\r\n",
    "        super().__init__()\r\n",
    "        #layers\r\n",
    "        self.inner_model = InnerModel(settings)\r\n",
    "        self.distance_layer = DistanceLayer()\r\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\",name=\"output_layer\")\r\n",
    "\r\n",
    "        #learned representations\r\n",
    "        self.repr_a = []\r\n",
    "        self.repr_b = []\r\n",
    "        self.cosine_similarity = 0\r\n",
    "\r\n",
    "    def call(self,inputs,training=False):\r\n",
    "        input_a = inputs[0]\r\n",
    "        input_b = inputs[1]\r\n",
    "        # print(f\"Outer model - call - input_a shape: {input_a.shape}\")\r\n",
    "        # print(f\"Outer model - call - input_b shape: {input_b.shape}\")\r\n",
    "\r\n",
    "        self.repr_a = self.inner_model(input_a, training=training)\r\n",
    "        self.repr_b = self.inner_model(input_b, training=training)\r\n",
    "        # print(f\"Outer model - call - repr_a shape: {self.repr_a.shape}\")\r\n",
    "        # print(f\"Outer model - call - repr_b shape: {self.repr_b.shape}\")\r\n",
    "        self.cosine_similarity = self.distance_layer(self.repr_a,self.repr_b,training=training)\r\n",
    "\r\n",
    "        # print(f\"Outer model - call - cosine shape: {self.cosine_similarity.shape}\")\r\n",
    "        #print(self.cosine_similarity)\r\n",
    "        #print(f\"Outer model - call - output shape: {self.output_layer(self.cosine_similarity,training=training).shape}\")\r\n",
    "        \r\n",
    "        return self.cosine_similarity #self.output_layer(self.cosine_similarity,training=training)\r\n",
    "\r\n",
    "    def train_step(self, data):\r\n",
    "        x,y = data\r\n",
    "        # print(f\"Outer model - train - x shape: {x}\")\r\n",
    "        # print(f\"Outer model - train - y shape: {y}\")\r\n",
    "\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            #predict the value\r\n",
    "            y_pred = self(x, training=True)\r\n",
    "            #compute the loss\r\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\r\n",
    "\r\n",
    "        #compute gradients\r\n",
    "        trainable_variables = self.trainable_variables\r\n",
    "        gradients = tape.gradient(loss, trainable_variables)\r\n",
    "        #update weights\r\n",
    "        self.optimizer.apply_gradients(zip(gradients,trainable_variables))\r\n",
    "        #update metrics\r\n",
    "        self.compiled_metrics.update_state(y, y_pred)\r\n",
    "        return {m.name: m.result() for m in self.metrics}\r\n",
    "\r\n",
    "    # def fit(self,train_data,**kwargs):\r\n",
    "    #     print(train_data)\r\n",
    "    #     super().fit(train_data,**kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "print(to_categorical(match).shape)\r\n",
    "print(to_categorical(match).reshape(len(match),-1).shape)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(307377, 2)\n",
      "(307377, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "match_seq = np.array(match)\r\n",
    "data = tf.data.Dataset.from_tensor_slices(((governor_seq,combination_seq),match_seq)).shuffle(buffer_size=50).batch(200)\r\n",
    "\r\n",
    "train_ratio = .6\r\n",
    "val_ratio = .2\r\n",
    "test_ratio = .2\r\n",
    "\r\n",
    "train_batches = int(len(data) * train_ratio)\r\n",
    "val_batches = int(len(data) * val_ratio)\r\n",
    "test_batches = int(len(data) * test_ratio)\r\n",
    "\r\n",
    "train_data = data.take(train_batches)\r\n",
    "test_data = data.skip(train_batches)\r\n",
    "val_data = test_data.take(val_batches)\r\n",
    "test_data = test_data.skip(test_batches)\r\n",
    "\r\n",
    "x_train = train_data.map(lambda feature, outcome: feature)\r\n",
    "y_train = train_data.map(lambda feature, outcome: outcome)\r\n",
    "\r\n",
    "x_val = val_data.map(lambda feature, outcome: feature)\r\n",
    "y_val = val_data.map(lambda feature, outcome: outcome)\r\n",
    "\r\n",
    "x_test = test_data.map(lambda feature, outcome: feature)\r\n",
    "y_test = test_data.map(lambda feature, outcome: outcome)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "inner_model_settings = InnerModelSettings(\r\n",
    "    n_embedding_dims = 64, #len(tk.word_index),\r\n",
    "    n_gru = 10,\r\n",
    "    n_dense = 10\r\n",
    ")\r\n",
    "\r\n",
    "model = OuterModel(inner_model_settings)\r\n",
    "\r\n",
    "model.compile(\r\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\r\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\r\n",
    "    metrics=['accuracy'],\r\n",
    "#    run_eagerly=True\r\n",
    ")\r\n",
    "\r\n",
    "model.fit(\r\n",
    "    train_data,\r\n",
    "    batch_size = 1000,\r\n",
    "    epochs = 50,\r\n",
    "    validation_data = val_data,\r\n",
    "    verbose=1\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "922/922 [==============================] - 23s 19ms/step - loss: 0.3211 - accuracy: 0.0108 - val_loss: 0.8265 - val_accuracy: 0.0195\n",
      "Epoch 2/50\n",
      "922/922 [==============================] - 16s 18ms/step - loss: 0.2786 - accuracy: 0.0130 - val_loss: 0.6237 - val_accuracy: 0.0033\n",
      "Epoch 3/50\n",
      "921/922 [============================>.] - ETA: 0s - loss: 0.2660 - accuracy: 0.0174"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-7dd15518ed53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1212\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1214\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1215\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1487\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1489\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1490\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp_tensor\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "class ShapeChecker():\r\n",
    "  def __init__(self):\r\n",
    "    # Keep a cache of every axis-name seen\r\n",
    "    self.shapes = {}\r\n",
    "\r\n",
    "  def __call__(self, tensor, names, broadcast=False):\r\n",
    "    if not tf.executing_eagerly():\r\n",
    "      return\r\n",
    "\r\n",
    "    if isinstance(names, str):\r\n",
    "      names = (names,)\r\n",
    "\r\n",
    "    shape = tf.shape(tensor)\r\n",
    "    rank = tf.rank(tensor)\r\n",
    "\r\n",
    "    if rank != len(names):\r\n",
    "      raise ValueError(f'Rank mismatch:\\n'\r\n",
    "                       f'    found {rank}: {shape.numpy()}\\n'\r\n",
    "                       f'    expected {len(names)}: {names}\\n')\r\n",
    "\r\n",
    "    for i, name in enumerate(names):\r\n",
    "      if isinstance(name, int):\r\n",
    "        old_dim = name\r\n",
    "      else:\r\n",
    "        old_dim = self.shapes.get(name, None)\r\n",
    "      new_dim = shape[i]\r\n",
    "\r\n",
    "      if (broadcast and new_dim == 1):\r\n",
    "        continue\r\n",
    "\r\n",
    "      if old_dim is None:\r\n",
    "        # If the axis name is new, add its length to the cache.\r\n",
    "        self.shapes[name] = new_dim\r\n",
    "        continue\r\n",
    "\r\n",
    "      if new_dim != old_dim:\r\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\r\n",
    "                         f\"    found: {new_dim}\\n\"\r\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "def compare_representations(input_a, input_b, debug=False):\r\n",
    "    inner_model_settings = InnerModelSettings(\r\n",
    "        n_embedding_dims = len(tk.word_index),\r\n",
    "        n_gru = 10,\r\n",
    "        n_dense = 10\r\n",
    "    )\r\n",
    "    outer_model = OuterModel(inner_model_settings)\r\n",
    "    outer_model((input_a.reshape(-1,len(input_a)),input_b.reshape(-1,len(input_b))))\r\n",
    "\r\n",
    "    if debug:\r\n",
    "        print(f\"Representation of A: {outer_model.repr_a}\")\r\n",
    "        print(f\"Representation of B: {outer_model.repr_b}\")\r\n",
    "        print(f\"Similarity: {outer_model.cosine_similarity}\")\r\n",
    "\r\n",
    "    return outer_model.cosine_similarity, (outer_model.repr_a,outer_model.repr_a)\r\n",
    "\r\n",
    "governor_input = governor_seq.reshape((1,)+governor_seq.shape)\r\n",
    "combination_input = combination_seq.reshape((1,) +combination_seq.shape)\r\n",
    "\r\n",
    "print(f\"Comparing '{governors_list[0]}' and '{combination_list[0]}'\")\r\n",
    "\r\n",
    "similarity, representations = compare_representations(\r\n",
    "    governor_seq[0],\r\n",
    "    combination_seq[0],\r\n",
    "    True\r\n",
    ")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Comparing 'a.p. lutali' and 'a.p. lutali'\n",
      "Representation of A: [[-0.0379  0.0017  0.0177 -0.0266  0.0065  0.0315 -0.0477 -0.0332 -0.0195\n",
      "   0.0385]]\n",
      "Representation of B: [[-0.0379  0.0017  0.0177 -0.0266  0.0065  0.0315 -0.0477 -0.0332 -0.0195\n",
      "   0.0385]]\n",
      "Similarity: [1.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "source": [
    "import random\r\n",
    "from itertools import filterfalse\r\n",
    "random.seed(20211808)\r\n",
    "\r\n",
    "def form_data(input_a,input_b,match, ind):\r\n",
    "    return zip(input_a[ind],input_b[ind]), match[ind]\r\n",
    "\r\n",
    "def prepare_train_test_data(governor_seq,combination_seq,match,ratio):\r\n",
    "    if len(governor_seq) != len(combination_seq):\r\n",
    "        return None\r\n",
    "    n_records = len(governor_seq)\r\n",
    "    train_indices = random.sample(range(0,n_records),int(float(n_records) * ratio))\r\n",
    "    print(f\"selected {len(train_indices)} for training\")\r\n",
    "    test_indices = list(filterfalse(train_indices.__contains__, list(range(n_records))))\r\n",
    "    [x for x in range(n_records) if x not in train_indices]\r\n",
    "    print(f\"selected {len(test_indices)} for testing\")\r\n",
    "\r\n",
    "    x_train, y_train = form_data(governor_seq,combination_seq,match,train_indices)\r\n",
    "    x_test, y_test = form_data(governor_seq,combination_seq,match,test_indices)\r\n",
    "    \r\n",
    "    return x_train, y_train, x_test, y_test\r\n",
    "\r\n",
    "x_train, y_train, x_test, y_test = prepare_train_test_data(governor_seq,combination_seq,match,.6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inner_model_settings = InnerModelSettings(\r\n",
    "    n_embedding_dims = len(tk.word_index),\r\n",
    "    n_gru = 10,\r\n",
    "    n_dense = 10\r\n",
    ")\r\n",
    "\r\n",
    "model_1 = OuterModel(inner_model_settings)\r\n",
    "model_1.compile()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "print(governor_seq.shape)\r\n",
    "print(governor_seq.reshape((1,)+governor_seq.shape).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(307377, 11)\n",
      "(1, 307377, 11)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "def text_from_ids(ids,dict=tk.word_index):\r\n",
    "    inv_dict = {v: k for k,v in tk.word_index.items()}\r\n",
    "    char_list = []\r\n",
    "    for id in ids:\r\n",
    "        if id not in inv_dict:\r\n",
    "            char = \"_\"\r\n",
    "        else:\r\n",
    "            char = inv_dict[id]\r\n",
    "        char_list.append(char)\r\n",
    "\r\n",
    "    return(\"\".join(char_list))\r\n",
    "\r\n",
    "print(text_from_ids(governor_seq[0]))\r\n",
    "print(text_from_ids(combination_seq[1]))\r\n",
    "\r\n",
    "print(governor_seq[0])\r\n",
    "print(combination_seq[1])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a.p. lutali\n",
      "lutali a.p.\n",
      "[ 4. 25. 23. 25.  2.  7. 20. 13.  4.  7.  9.]\n",
      "[ 7. 20. 13.  4.  7.  9.  2.  4. 25. 23. 25.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "def count_elements(lst):\r\n",
    "    unique_elements = sorted(set(lst))\r\n",
    "    return_dict = {}\r\n",
    "    for el in unique_elements:\r\n",
    "        return_dict.update({int(el):len(lst[lst==el])})\r\n",
    "\r\n",
    "    return return_dict\r\n",
    "\r\n",
    "print(count_elements(governor_seq[0]))\r\n",
    "print(count_elements(combination_seq[0]))\r\n",
    "print(count_elements(combination_seq[1]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{2: 1, 4: 2, 7: 2, 9: 1, 13: 1, 20: 1, 23: 1, 25: 2}\n",
      "{2: 1, 4: 2, 7: 2, 9: 1, 13: 1, 20: 1, 23: 1, 25: 2}\n",
      "{2: 1, 4: 2, 7: 2, 9: 1, 13: 1, 20: 1, 23: 1, 25: 2}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "t1 = [[1, 2, 3],\r\n",
    "      [4, 5, 6]]\r\n",
    "t2 = tf.reshape(t1, [6])\r\n",
    "print(tf.reshape(t2,shape=(-1,len(t2))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([[1 2 3 4 5 6]], shape=(1, 6), dtype=int32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('nlp_tensor': conda)"
  },
  "interpreter": {
   "hash": "fcbba280eac50286b31d8cb86df25696dcc42d5d7558ed290f125a3371417a84"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}